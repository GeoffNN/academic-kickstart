@article{askari_lifted_2018,
 abstract = {We describe a novel family of models of multi- layer feedforward neural networks in which the activation functions are encoded via penalties in the training problem. Our approach is based on representing a non-decreasing activation function as the argmin of an appropriate convex optimiza- tion problem. The new framework allows for algo- rithms such as block-coordinate descent methods to be applied, in which each step is composed of a simple (no hidden layer) supervised learning problem that is parallelizable across data points and/or layers. Experiments indicate that the pro- posed models provide excellent initial guesses for weights for standard neural networks. In addi- tion, the model provides avenues for interesting extensions, such as robustness against noisy in- puts and optimizing over parameters in activation functions.},
 author = {Askari, Armin and Negiar, Geoffrey and Sambharya, Rajiv and Ghaoui, Laurent El},
 copyright = {All rights reserved},
 file = {arXiv Fulltext PDF:/home/geoff/Zotero/storage/B9DHCAH6/Askari et al. - 2018 - Lifted Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/geoff/Zotero/storage/W4UMITF4/1805.html:text/html},
 journal = {arXiv:1805.01532 [cs, stat]},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 month = {June},
 note = {arXiv: 1805.01532},
 title = {Lifted Neural Networks},
 url = {http://arxiv.org/abs/1805.01532},
 urldate = {2020-04-14},
 year = {2018}
}

